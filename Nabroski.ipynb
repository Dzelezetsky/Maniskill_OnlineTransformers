{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "import gymnasium as gym\n",
    "from mani_skill.utils import gym_utils\n",
    "from mani_skill.utils.wrappers.flatten import FlattenActionSpaceWrapper\n",
    "from mani_skill.utils.wrappers.record import RecordEpisode\n",
    "from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv\n",
    "from dataclasses import dataclass\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/anaconda3/envs/rate/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/daniil/anaconda3/envs/rate/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env_kwargs = dict(obs_mode=\"state\", render_mode=\"rgb_array\", sim_backend=\"gpu\")\n",
    "env_kwargs[\"control_mode\"] = \"pd_joint_delta_pos\"\n",
    "    \n",
    "envs = gym.make('PickCube-v1', num_envs=50, reconfiguration_freq=None, **env_kwargs)\n",
    "\n",
    "rb = ReplayBuffer(envs , 50, contex, 1000000, 'cuda', 'cuda')\n",
    "\n",
    "rb.pos = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 10, 42]),\n",
       " torch.Size([512, 10, 42]),\n",
       " torch.Size([512, 8]),\n",
       " torch.Size([512]),\n",
       " torch.Size([512]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = rb.sample(512)\n",
    "\n",
    "batch.obs.shape, batch.next_obs.shape, batch.actions.shape , batch.rewards.shape , batch.dones.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "contex = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros(50,contex,42)\n",
    "next_obs = torch.zeros(50,contex,42)\n",
    "actions = torch.zeros(50,8)\n",
    "rewards = torch.zeros(50)\n",
    "dones = torch.zeros(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.add(obs, next_obs, actions, rewards, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(np.array(env.single_observation_space.shape).prod(), 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        # action rescaling\n",
    "        h, l = env.single_action_space.high, env.single_action_space.low\n",
    "        self.register_buffer(\"action_scale\", torch.tensor((h - l) / 2.0, dtype=torch.float32))\n",
    "        self.register_buffer(\"action_bias\", torch.tensor((h + l) / 2.0, dtype=torch.float32))\n",
    "        # will be saved in the state_dict\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = self.fc_logstd(x)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats\n",
    "\n",
    "        return mean, log_std\n",
    "\n",
    "    def get_eval_action(self, x):\n",
    "        x = self.backbone(x)\n",
    "        mean = self.fc_mean(x)\n",
    "        action = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action\n",
    "\n",
    "    def get_action(self, x):\n",
    "        mean, log_std = self(x)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean\n",
    "\n",
    "    def to(self, device):\n",
    "        self.action_scale = self.action_scale.to(device)\n",
    "        self.action_bias = self.action_bias.to(device)\n",
    "        return super().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, device, min_timescale=2.0, max_timescale=1e4):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        freqs = torch.arange(0, dim, min_timescale).to(self.device)\n",
    "        inv_freqs = max_timescale ** (-freqs / dim)\n",
    "        self.register_buffer(\"inv_freqs\", inv_freqs)\n",
    "\n",
    "    def forward(self, seq_len):\n",
    "        seq = torch.arange(seq_len - 1, -1, -1.0).to(self.device)\n",
    "        sinusoidal_inp = rearrange(seq, \"n -> n ()\") * rearrange(self.inv_freqs, \"d -> () d\")\n",
    "        pos_emb = torch.cat((sinusoidal_inp.sin(), sinusoidal_inp.cos()), dim=-1)\n",
    "        return pos_emb\n",
    "\n",
    "#######################################################################################################################################\n",
    "##########################################################  Gatings  #################################################################\n",
    "\n",
    "class GRUGate(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int, bg: float = 0.0):\n",
    "        \n",
    "        super(GRUGate, self).__init__()\n",
    "        self.Wr = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.Ur = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.Wz = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.Uz = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.Wg = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.Ug = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.bg = nn.Parameter(torch.full([input_dim], bg))  # bias\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        nn.init.xavier_uniform_(self.Wr.weight)\n",
    "        nn.init.xavier_uniform_(self.Ur.weight)\n",
    "        nn.init.xavier_uniform_(self.Wz.weight)\n",
    "        nn.init.xavier_uniform_(self.Uz.weight)\n",
    "        nn.init.xavier_uniform_(self.Wg.weight)\n",
    "        nn.init.xavier_uniform_(self.Ug.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"        \n",
    "        Arguments:\n",
    "            x {torch.tensor} -- First input\n",
    "            y {torch.tensor} -- Second input\n",
    "        Returns:\n",
    "            {torch.tensor} -- Output\n",
    "        \"\"\"\n",
    "        r = self.sigmoid(self.Wr(y) + self.Ur(x))\n",
    "        z = self.sigmoid(self.Wz(y) + self.Uz(x) - self.bg)\n",
    "        h = self.tanh(self.Wg(y) + self.Ug(torch.mul(r, x)))\n",
    "\n",
    "        # print(f'mean z: {z.mean()}')\n",
    "\n",
    "        return torch.mul(1 - z, x) + torch.mul(z, h) #, z.mean()\n",
    "\n",
    "#######################################################################################################################################\n",
    "######################################################## nano gpt modification ########################################################\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "    \n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.num_steps, config.num_steps))\n",
    "                                        .view(1, 1, config.num_steps, config.num_steps))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "        if config.use_gates:\n",
    "            self.skip_fn_1 = GRUGate(config.n_embd, 2.0)\n",
    "            self.skip_fn_2 = GRUGate(config.n_embd, 2.0)\n",
    "        else:\n",
    "            self.skip_fn_1 = lambda x, y: x + y\n",
    "            self.skip_fn_2 = lambda x, y: x + y\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.skip_fn_1(x, self.attn(self.ln_1(x)))\n",
    "        x = self.skip_fn_2(x, self.mlp(self.ln_2(x)))\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.pos_embedding = nn.Embedding(config.max_episode_steps, config.n_embd)\n",
    "\n",
    "        self.transformer_layers = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    \n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        t = x.shape[1]\n",
    "\n",
    "        device = x.device\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "        pos_emb = self.pos_embedding(pos) # position embeddings of shape (t, n_embd)\n",
    "        \n",
    "        x = self.drop(x + pos_emb)\n",
    "        for block in self.transformer_layers:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.transformer = GPT(args)\n",
    "        self.encoder = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        # action rescaling\n",
    "        h, l = env.single_action_space.high, env.single_action_space.low\n",
    "        self.register_buffer(\"action_scale\", torch.tensor((h - l) / 2.0, dtype=torch.float32))\n",
    "        self.register_buffer(\"action_bias\", torch.tensor((h + l) / 2.0, dtype=torch.float32))\n",
    "        # will be saved in the state_dict\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.head(x)\n",
    "        \n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = self.fc_logstd(x)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats\n",
    "\n",
    "        return mean, log_std\n",
    "\n",
    "    def get_eval_action(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.head(x)\n",
    "        \n",
    "        mean = self.fc_mean(x)\n",
    "        action = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action\n",
    "\n",
    "    def get_action(self, x):\n",
    "        mean, log_std = self(x)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean\n",
    "\n",
    "    def to(self, device):\n",
    "        self.action_scale = self.action_scale.to(device)\n",
    "        self.action_bias = self.action_bias.to(device)\n",
    "        return super().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRITIC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    '''\n",
    "    NIK\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, env, args, transformer):\n",
    "        super().__init__()\n",
    "        self.transformer = GPT(args)\n",
    "        self.encoder = nn.Linear(np.array(env.single_observation_space.shape).prod(), args.n_embd)\n",
    "        self.fc1 = nn.Linear(np.array(args.n_embd + np.prod(env.single_action_space.shape)), args.head_dim)\n",
    "        self.fc2 = nn.Linear(args.head_dim, args.head_dim)\n",
    "        self.fc3 = nn.Linear(args.head_dim, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = self.encoder(x)\n",
    "        x = self.transformer(x)[:, -1, :]\n",
    "\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SoftQNetwork(nn.Module):\n",
    "    '''\n",
    "    MANI\n",
    "    '''\n",
    "    def __init__(self, env, args):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], 1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    '''\n",
    "    Q-network for Transformer-based maniskill tasks\n",
    "    '''\n",
    "    def __init__(self, env, args):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer = GPT(args)\n",
    "        self.encoder = nn.Linear(np.array(env.single_observation_space.shape).prod(), args.n_embd)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(args.n_embd + np.prod(env.single_action_space.shape), 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, a):                # x = (batch,cont,s_d)   a = (batch,a_d)\n",
    "        x = self.encoder(x)                 # x = (batch,cont,n_embd)\n",
    "        x = self.transformer(x)[:, -1, :]   # x = (batch,n_embd)\n",
    "        x = torch.cat([x, a], 1)            # x = (batch,n_embd+a_d)\n",
    "        return self.net(x)                  # x = (batch,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
