{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "import gymnasium as gym\n",
    "from mani_skill.utils import gym_utils\n",
    "from mani_skill.utils.wrappers.flatten import FlattenActionSpaceWrapper\n",
    "from mani_skill.utils.wrappers.record import RecordEpisode\n",
    "from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv\n",
    "from dataclasses import dataclass\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_slice(observations, context, elapsed_steps, for_rb):\n",
    "    \"\"\"Возвращает срез observations с паддингом при недостатке шагов.\"\"\"\n",
    "    num_envs, seq_len, state_dim = observations.shape\n",
    "    \n",
    "    observations2RB = []\n",
    "    next_observations2RB = []\n",
    "\n",
    "    if seq_len >= context:\n",
    "        for env_index in range(num_envs):\n",
    "            if env_index in tracker.step_counts.keys(): # если эта среда нуждается в аккуратном срезе\n",
    "                steps_since_reset = tracker.get_info(env_index)  # узнаём сколько шагов простепали после ресета\n",
    "                padding_size = context - steps_since_reset       # считаем сколько шагов надо допадить\n",
    "                padding = observations[env_index, -steps_since_reset, :].unsqueeze(0).repeat(padding_size, 1) # формируем паддинг\n",
    "                valid_states = observations[env_index, -steps_since_reset:, :]\n",
    "                next_obs = torch.cat([padding, valid_states], dim=0)  # работает если после ресета прошло мин 2 степа\n",
    "                obs = torch.cat([ padding[0].unsqueeze(0), padding, valid_states[:-1,:] ], dim=0)# работает если после ресета прошло мин 2 степа\n",
    "                next_observations2RB.append(next_obs)\n",
    "                observations2RB.append(obs)\n",
    "            elif env_index not in tracker.step_counts.keys():\n",
    "                next_obs = observations[env_index, -context:, :] # всегда cont, s_d\n",
    "                obs = torch.cat([ next_obs[0,:].unsqueeze(0), next_obs[:-1,:] ])   # а точно ли я должен next_obs[0,:] добавлять или надо просто другой срез сделать? \n",
    "                next_observations2RB.append(next_obs)\n",
    "                observations2RB.append(obs)\n",
    "        \n",
    "        return torch.stack(observations2RB), torch.stack(next_observations2RB)    \n",
    "    \n",
    "    else:                \n",
    "        padding_size = context - seq_len\n",
    "        padding = observations[:, 0, :].unsqueeze(1).repeat(1, padding_size, 1)  # паддинг первым состоянием\n",
    "\n",
    "        \n",
    "        next_obs = torch.cat([padding, observations], dim=1)\n",
    "\n",
    "        obs = torch.cat([\n",
    "                observations[:, 0, :].unsqueeze(1),  # начальное состояние\n",
    "                padding,\n",
    "                observations[:,:-1, :]\n",
    "            ], dim=1)\n",
    "\n",
    "    return obs, next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3715526329.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[290], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    obs = observations[]\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def smart_slice(observations, context, elapsed_steps, for_rb=True):\n",
    "    num_envs, seq_len, state_dim = observations.shape\n",
    "    \n",
    "    observations2RB = []\n",
    "    next_observations2RB = []\n",
    "    \n",
    "    if for_rb: # если режем для rb\n",
    "        for env_index in range(num_envs): # пробегаем по средам\n",
    "            \n",
    "            steps_since_reset = elapsed_steps[env_index]\n",
    "            if steps_since_reset >= context:  # этого условия достаточно что бы без пад-гов сделать s и s'\n",
    "                obs = observations[env_index, -context-1:-1, :]\n",
    "                next_obs = observations[env_index, -context:, :]\n",
    "            \n",
    "            elif 0 < steps_since_reset < context:                \n",
    "                padding_size = context - steps_since_reset \n",
    "                padding = observations[env_index, -steps_since_reset-1, :].unsqueeze(0).repeat(padding_size, 1) \n",
    "                valid_states = observations[env_index, -steps_since_reset-1:, :]\n",
    "                intermediate_obs = torch.cat([padding, valid_states], dim=0) \n",
    "                obs = intermediate_obs[env_index, -context-1:-1, :]\n",
    "                next_obs = intermediate_obs[env_index, -context:, :]\n",
    "            \n",
    "            elif  steps_since_reset == 0:  # это обманка, на самом деле мы не начали с нового сост-я а видим 51й кадр\n",
    "                obs = observations[env_index, -context-1:-1, :]\n",
    "                next_obs = observations[env_index, -context:, :]\n",
    "                \n",
    "            observations2RB.append(obs)\n",
    "            next_observations2RB.append(next_obs)  \n",
    "            \n",
    "        return torch.stack(observations2RB), torch.stack(next_observations2RB)\n",
    "                \n",
    "    if not for_rb:\n",
    "        for env_index in range(num_envs): # пробегаем по средам\n",
    "            \n",
    "            steps_since_reset = elapsed_steps[env_index]\n",
    "            if steps_since_reset >= context-1:                  # самый позитивый сценарий, просто нарезаем и не паримся\n",
    "                next_obs = observations[env_index, -context:, :]\n",
    "                \n",
    "            elif steps_since_reset < context-1: \n",
    "                padding_size = context - steps_since_reset - 1\n",
    "                padding = observations[env_index, -steps_since_reset-1, :].unsqueeze(0).repeat(padding_size, 1)\n",
    "                valid_states = observations[env_index, -steps_since_reset-1:, :]\n",
    "                next_obs = torch.cat([padding, valid_states], dim=0)\n",
    "             \n",
    "            next_observations2RB.append(next_obs)\n",
    "        \n",
    "        return torch.stack(next_observations2RB)\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list.append() takes exactly one argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[357], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: list.append() takes exactly one argument (0 given)"
     ]
    }
   ],
   "source": [
    "[1].append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [0]   # interm=[0,0,0,0,1]   s=[0,0,0,0,1] s'=[0,0,0,1,2]\n",
    "c = 5\n",
    "ssr = 0\n",
    "padding_size = c - ssr - 1\n",
    "\n",
    "padding = data[-ssr-1]\n",
    "valid_states = data[-ssr-1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs = dict(obs_mode=\"state\", render_mode=\"rgb_array\", sim_backend=\"gpu\")\n",
    "env_kwargs[\"control_mode\"] = \"pd_joint_delta_pos\"\n",
    "    \n",
    "envs = gym.make('PickCube-v1', num_envs=2, reconfiguration_freq=None, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = ManiSkillVectorEnv(envs, num_envs=2, ignore_terminations=True, record_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.1150e-02,  4.0779e-01, -2.2373e-02, -1.9569e+00,  1.8996e-02,\n",
       "           2.3441e+00,  8.0386e-01,  4.0000e-02,  4.0000e-02,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.2717e-03,\n",
       "          -5.5752e-03,  1.6226e-01, -2.2465e-03,  9.9973e-01, -2.0674e-02,\n",
       "          -1.0255e-02,  3.7426e-02, -5.1588e-02,  2.8513e-02,  3.6168e-02,\n",
       "           7.2479e-03,  2.0000e-02,  9.9970e-01,  0.0000e+00,  0.0000e+00,\n",
       "           2.4371e-02,  3.8440e-02,  1.2823e-02, -1.4226e-01,  1.2584e-03,\n",
       "          -5.8836e-02,  8.5133e-03],\n",
       "         [-1.9129e-02,  3.8696e-01, -3.2823e-02, -2.0038e+00, -7.6136e-03,\n",
       "           2.3730e+00,  7.6408e-01,  4.0000e-02,  4.0000e-02,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3633e-02,\n",
       "          -3.2608e-02,  1.5825e-01,  8.4968e-03,  9.9986e-01, -1.1397e-02,\n",
       "          -9.0978e-03, -9.2967e-02,  4.1043e-02,  1.8076e-01, -7.7547e-02,\n",
       "          -4.3664e-02,  2.0000e-02,  9.0595e-01,  0.0000e+00,  0.0000e+00,\n",
       "          -4.2339e-01, -6.3914e-02, -1.1055e-02, -1.3825e-01, -1.5420e-02,\n",
       "           8.4706e-02,  1.6076e-01]], device='cuda:0'),\n",
       " {'elapsed_steps': tensor([0, 0], device='cuda:0', dtype=torch.int32),\n",
       "  'success': tensor([False, False], device='cuda:0'),\n",
       "  'is_obj_placed': tensor([False, False], device='cuda:0'),\n",
       "  'is_robot_static': tensor([True, True], device='cuda:0'),\n",
       "  'is_grasped': tensor([False, False], device='cuda:0'),\n",
       "  'reconfigure': False})"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'elapsed_steps': tensor([0, 0], device='cuda:0', dtype=torch.int32), 'success': tensor([False, False], device='cuda:0'), 'is_obj_placed': tensor([False, False], device='cuda:0'), 'is_robot_static': tensor([True, True], device='cuda:0'), 'is_grasped': tensor([False, False], device='cuda:0'), 'reconfigure': False, 'final_observation': tensor([[-1.6021e-01,  2.7413e-01, -1.8904e-01, -2.0245e+00, -1.7902e-01,\n",
      "          2.5964e+00,  7.5553e-01,  2.9946e-02,  2.9946e-02,  1.8216e-01,\n",
      "         -5.4423e-01,  5.4502e-01,  6.7879e-01,  1.3636e-01,  8.2061e-02,\n",
      "         -4.2582e-01, -1.8103e-02, -1.8403e-02,  0.0000e+00, -3.2043e-03,\n",
      "         -2.2896e-01,  2.4953e-01,  1.0838e-01,  9.8215e-01, -8.1450e-02,\n",
      "          1.3042e-01,  8.4882e-02,  5.3333e-02,  6.1865e-02,  4.0793e-02,\n",
      "          2.2261e-02,  2.0000e-02,  5.0411e-02,  5.4773e-06,  2.2165e-06,\n",
      "         -9.9873e-01,  4.3997e-02,  2.5122e-01, -2.2953e-01,  4.4089e-02,\n",
      "          3.1072e-02,  4.1865e-02],\n",
      "        [ 5.1923e-02,  2.5373e-01,  5.0935e-02, -1.7503e+00, -2.1607e-01,\n",
      "          2.2977e+00,  7.9977e-01,  5.9719e-03,  5.9678e-03,  5.4906e-01,\n",
      "          4.7268e-01, -1.3447e-01, -5.2383e-02, -1.9145e-01,  4.0753e-01,\n",
      "         -3.8345e-02, -1.3191e-01, -1.2741e-01,  0.0000e+00,  7.2094e-02,\n",
      "          4.8194e-02,  3.5798e-01,  7.5333e-02,  9.8055e-01,  1.0081e-01,\n",
      "          1.5062e-01, -2.6424e-02, -9.6298e-02,  2.6583e-01,  2.5254e-02,\n",
      "          5.0516e-02,  2.0000e-02,  5.3298e-02,  8.5768e-06,  1.5284e-06,\n",
      "          9.9858e-01, -4.6840e-02,  2.3212e-03, -3.3798e-01, -5.1678e-02,\n",
      "         -1.4681e-01,  2.4583e-01]], device='cuda:0'), 'final_info': {'elapsed_steps': tensor([50, 50], device='cuda:0', dtype=torch.int32), 'success': tensor([False, False], device='cuda:0'), 'is_obj_placed': tensor([False, False], device='cuda:0'), 'is_robot_static': tensor([False, False], device='cuda:0'), 'is_grasped': tensor([False, False], device='cuda:0'), 'episode': {'success_once': tensor([False, False], device='cuda:0'), 'return': tensor([1.9848, 1.5692], device='cuda:0'), 'episode_len': tensor([50, 50], device='cuda:0', dtype=torch.int32), 'reward': tensor([0.0397, 0.0314], device='cuda:0'), 'success_at_end': tensor([False, False], device='cuda:0')}}, '_final_info': tensor([True, True], device='cuda:0'), '_final_observation': tensor([True, True], device='cuda:0'), '_elapsed_steps': tensor([True, True], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "o, r, term, tru, info = envs.step(envs.action_space.sample())\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True], device='cuda:0')"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True], device='cuda:0')"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru | term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = False\n",
    "t = 1\n",
    "while not flag:\n",
    "    obs = envs.step(envs.action_space.sample())\n",
    "    t += 1\n",
    "    flag = obs[3].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[-1]['elapsed_steps'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[3].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/anaconda3/envs/rate/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/daniil/anaconda3/envs/rate/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env_kwargs = dict(obs_mode=\"state\", render_mode=\"rgb_array\", sim_backend=\"gpu\")\n",
    "env_kwargs[\"control_mode\"] = \"pd_joint_delta_pos\"\n",
    "    \n",
    "envs = gym.make('PickCube-v1', num_envs=50, reconfiguration_freq=None, **env_kwargs)\n",
    "\n",
    "rb = ReplayBuffer(envs , 50, contex, 1000000, 'cuda', 'cuda')\n",
    "\n",
    "rb.pos = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 10, 42]),\n",
       " torch.Size([512, 10, 42]),\n",
       " torch.Size([512, 8]),\n",
       " torch.Size([512]),\n",
       " torch.Size([512]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = rb.sample(512)\n",
    "\n",
    "batch.obs.shape, batch.next_obs.shape, batch.actions.shape , batch.rewards.shape , batch.dones.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "contex = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros(50,contex,42)\n",
    "next_obs = torch.zeros(50,contex,42)\n",
    "actions = torch.zeros(50,8)\n",
    "rewards = torch.zeros(50)\n",
    "dones = torch.zeros(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.add(obs, next_obs, actions, rewards, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(np.array(env.single_observation_space.shape).prod(), 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        # action rescaling\n",
    "        h, l = env.single_action_space.high, env.single_action_space.low\n",
    "        self.register_buffer(\"action_scale\", torch.tensor((h - l) / 2.0, dtype=torch.float32))\n",
    "        self.register_buffer(\"action_bias\", torch.tensor((h + l) / 2.0, dtype=torch.float32))\n",
    "        # will be saved in the state_dict\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = self.fc_logstd(x)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats\n",
    "\n",
    "        return mean, log_std\n",
    "\n",
    "    def get_eval_action(self, x):\n",
    "        x = self.backbone(x)\n",
    "        mean = self.fc_mean(x)\n",
    "        action = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action\n",
    "\n",
    "    def get_action(self, x):\n",
    "        mean, log_std = self(x)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean\n",
    "\n",
    "    def to(self, device):\n",
    "        self.action_scale = self.action_scale.to(device)\n",
    "        self.action_bias = self.action_bias.to(device)\n",
    "        return super().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, device, min_timescale=2.0, max_timescale=1e4):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        freqs = torch.arange(0, dim, min_timescale).to(self.device)\n",
    "        inv_freqs = max_timescale ** (-freqs / dim)\n",
    "        self.register_buffer(\"inv_freqs\", inv_freqs)\n",
    "\n",
    "    def forward(self, seq_len):\n",
    "        seq = torch.arange(seq_len - 1, -1, -1.0).to(self.device)\n",
    "        sinusoidal_inp = rearrange(seq, \"n -> n ()\") * rearrange(self.inv_freqs, \"d -> () d\")\n",
    "        pos_emb = torch.cat((sinusoidal_inp.sin(), sinusoidal_inp.cos()), dim=-1)\n",
    "        return pos_emb\n",
    "\n",
    "#######################################################################################################################################\n",
    "##########################################################  Gatings  #################################################################\n",
    "\n",
    "class GRUGate(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int, bg: float = 0.0):\n",
    "        \n",
    "        super(GRUGate, self).__init__()\n",
    "        self.Wr = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.Ur = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.Wz = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.Uz = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.Wg = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.Ug = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.bg = nn.Parameter(torch.full([input_dim], bg))  # bias\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        nn.init.xavier_uniform_(self.Wr.weight)\n",
    "        nn.init.xavier_uniform_(self.Ur.weight)\n",
    "        nn.init.xavier_uniform_(self.Wz.weight)\n",
    "        nn.init.xavier_uniform_(self.Uz.weight)\n",
    "        nn.init.xavier_uniform_(self.Wg.weight)\n",
    "        nn.init.xavier_uniform_(self.Ug.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"        \n",
    "        Arguments:\n",
    "            x {torch.tensor} -- First input\n",
    "            y {torch.tensor} -- Second input\n",
    "        Returns:\n",
    "            {torch.tensor} -- Output\n",
    "        \"\"\"\n",
    "        r = self.sigmoid(self.Wr(y) + self.Ur(x))\n",
    "        z = self.sigmoid(self.Wz(y) + self.Uz(x) - self.bg)\n",
    "        h = self.tanh(self.Wg(y) + self.Ug(torch.mul(r, x)))\n",
    "\n",
    "        # print(f'mean z: {z.mean()}')\n",
    "\n",
    "        return torch.mul(1 - z, x) + torch.mul(z, h) #, z.mean()\n",
    "\n",
    "#######################################################################################################################################\n",
    "######################################################## nano gpt modification ########################################################\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "    \n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.num_steps, config.num_steps))\n",
    "                                        .view(1, 1, config.num_steps, config.num_steps))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "        if config.use_gates:\n",
    "            self.skip_fn_1 = GRUGate(config.n_embd, 2.0)\n",
    "            self.skip_fn_2 = GRUGate(config.n_embd, 2.0)\n",
    "        else:\n",
    "            self.skip_fn_1 = lambda x, y: x + y\n",
    "            self.skip_fn_2 = lambda x, y: x + y\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.skip_fn_1(x, self.attn(self.ln_1(x)))\n",
    "        x = self.skip_fn_2(x, self.mlp(self.ln_2(x)))\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.pos_embedding = nn.Embedding(config.max_episode_steps, config.n_embd)\n",
    "\n",
    "        self.transformer_layers = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    \n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        t = x.shape[1]\n",
    "\n",
    "        device = x.device\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "        pos_emb = self.pos_embedding(pos) # position embeddings of shape (t, n_embd)\n",
    "        \n",
    "        x = self.drop(x + pos_emb)\n",
    "        for block in self.transformer_layers:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.transformer = GPT(args)\n",
    "        self.encoder = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        # action rescaling\n",
    "        h, l = env.single_action_space.high, env.single_action_space.low\n",
    "        self.register_buffer(\"action_scale\", torch.tensor((h - l) / 2.0, dtype=torch.float32))\n",
    "        self.register_buffer(\"action_bias\", torch.tensor((h + l) / 2.0, dtype=torch.float32))\n",
    "        # will be saved in the state_dict\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.head(x)\n",
    "        \n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = self.fc_logstd(x)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats\n",
    "\n",
    "        return mean, log_std\n",
    "\n",
    "    def get_eval_action(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.head(x)\n",
    "        \n",
    "        mean = self.fc_mean(x)\n",
    "        action = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action\n",
    "\n",
    "    def get_action(self, x):\n",
    "        mean, log_std = self(x)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean\n",
    "\n",
    "    def to(self, device):\n",
    "        self.action_scale = self.action_scale.to(device)\n",
    "        self.action_bias = self.action_bias.to(device)\n",
    "        return super().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRITIC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    '''\n",
    "    NIK\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, env, args, transformer):\n",
    "        super().__init__()\n",
    "        self.transformer = GPT(args)\n",
    "        self.encoder = nn.Linear(np.array(env.single_observation_space.shape).prod(), args.n_embd)\n",
    "        self.fc1 = nn.Linear(np.array(args.n_embd + np.prod(env.single_action_space.shape)), args.head_dim)\n",
    "        self.fc2 = nn.Linear(args.head_dim, args.head_dim)\n",
    "        self.fc3 = nn.Linear(args.head_dim, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = self.encoder(x)\n",
    "        x = self.transformer(x)[:, -1, :]\n",
    "\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SoftQNetwork(nn.Module):\n",
    "    '''\n",
    "    MANI\n",
    "    '''\n",
    "    def __init__(self, env, args):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], 1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    '''\n",
    "    Q-network for Transformer-based maniskill tasks\n",
    "    '''\n",
    "    def __init__(self, env, args):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer = GPT(args)\n",
    "        self.encoder = nn.Linear(np.array(env.single_observation_space.shape).prod(), args.n_embd)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(args.n_embd + np.prod(env.single_action_space.shape), 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, a):                # x = (batch,cont,s_d)   a = (batch,a_d)\n",
    "        x = self.encoder(x)                 # x = (batch,cont,n_embd)\n",
    "        x = self.transformer(x)[:, -1, :]   # x = (batch,n_embd)\n",
    "        x = torch.cat([x, a], 1)            # x = (batch,n_embd+a_d)\n",
    "        return self.net(x)                  # x = (batch,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
